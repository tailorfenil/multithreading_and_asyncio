# -*- coding: utf-8 -*-
"""Threading_and_asyncIO.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rtxjE3iAl8slusl8YTpKpswfElOgmpcK
"""

import time
from urllib.parse import urlparse as urlparser
from typing import Set  # Import Set from typing module

# Define test "webs" as graphs
test_graph_1 = {
    "http://site.com/a": ["http://site.com/b", "http://site.com/c","http://site.com/d"],
    "http://site.com/b": ["http://site.com/d"],
    "http://site.com/c": [],
    "http://site.com/d": []
}

test_graph_2 = {
    "http://site.com/a": ["http://site.com/b", "http://othersite.com/e"],
    "http://site.com/b": ["http://site.com/c"],
    "http://site.com/c": [],
    "http://othersite.com/e": ["http://othersite.com/f"],
    "http://othersite.com/f": []
}

class htmlparser:

  def __init__(self,web_crawler_map):
    self.web_crawler_map = web_crawler_map

  def parse(self,url:str) -> list[str]:
    return self.web_crawler_map.get(url,[])

class sequential_crawl:
  def __init__(self,url:str,htmlparser):
    self.url = url
    self.htmlparser = htmlparser
    self.visited = set()
    self.hostname = urlparser(url).hostname

  def crawl(self,url:str):
    if url in self.visited or urlparser(url).hostname != self.hostname:
      return
    self.visited.add(url)
    print(f"Crawling {url}")
    for child in self.htmlparser.parse(url):
      self.crawl(child)

  def run(self,start_url:str) -> Set[str]:
    self.crawl(start_url)
    return self.visited

st_time = time.time()
crawl = sequential_crawl("http://site.com/a",htmlparser(test_graph_1))
visited = crawl.run("http://site.com/a")
end_time = time.time()
print(f"Time taken: {end_time - st_time} seconds")

import threading
class crawl_threading:
  def __init__(self,url:str,htmlparser) -> None:
    self.url=url
    self.visited=set()
    self.htmlparser = htmlparser
    self.hostname = urlparser(url).hostname

  def crawl(self,url:str):
    if url in self.visited or urlparser(url).hostname != self.hostname:
      return
    self.visited.add(url)
    print(f"Crawling {url}")

    threads=[]
    for child in self.htmlparser.parse(url):
      thread = threading.Thread(target=self.crawl,args=(child,))
      thread.start()
      threads.append(thread)

    for thread in threads:
      thread.join()

crawlthreads = crawl_threading("http://site.com/a",htmlparser(test_graph_1))
st_time = time.time()
visite= crawlthreads.crawl("http://site.com/a")
end_time = time.time()
print(f"Time taken: {end_time - st_time} seconds")
print(visited)

class crawl_thread_safe:
  def __init__(self,url:str,htmlparser) -> None:
    self.url=url
    self.visited=set()
    self.htmlparser = htmlparser
    self.hostname = urlparser(url).hostname
    self.lock = threading.Lock()

  def crawl(self,url:str):
    with self.lock:
      if url in self.visited or urlparser(url).hostname != self.hostname:
        return
      self.visited.add(url)
      print(f"Crawling {url}")

    threads=[]

    for child in self.htmlparser.parse(url):
      thread = threading.Thread(target=self.crawl,args=(child,))
      thread.start()
      threads.append(thread)

    for thread in threads:
      thread.join()

  def run(self,start_url:str) -> Set[str]:
    self.crawl(start_url)
    return self.visited

st_time = time.time()
crawlthreadsafe = crawl_thread_safe("http://site.com/a",htmlparser(test_graph_1))
visited = crawlthreadsafe.run("http://site.com/a")
end_time = time.time()
print(f"Time taken: {end_time - st_time} seconds")
print(visited)

class crawl_thread_safe_concurrrency_limit:
  def __init__(self,url:str,htmlparser,max_threads:int=10) -> None:
    self.url=url
    self.visited=set()
    self.htmlparser = htmlparser
    self.hostname = urlparser(url).hostname
    self.lock= threading.Lock()
    self.semaphore = threading.Semaphore(max_threads)

  def crawl(self,url:str):
    with self.semaphore:
      with self.lock:
        if url in self.visited or urlparser(url).hostname != self.hostname:
          return
        self.visited.add(url)
        print(f"Crawling {url}")

      threads = []
      for child in self.htmlparser.parse(url):
        thread = threading.Thread(target=self.crawl,args=(child,))
        thread.start()
        threads.append(thread)

      for thread in threads:
        thread.join()

crawl_concurrency_limit = crawl_thread_safe_concurrrency_limit("http://site.com/a",htmlparser(test_graph_1))
st_time = time.time()
visited = crawl_concurrency_limit.crawl("http://site.com/a")
end_time = time.time()
print(f"Time taken: {end_time - st_time} seconds")
print(visited)

from concurrent.futures import ThreadPoolExecutor, wait
class crawlthreadingwithThreadPool:
  def __init__(self,url,htmlparser,max_threads:int=10):
    self.url=url
    self.htmlparser= htmlparser
    self.visited=set()
    self.hostname = urlparser(url).hostname
    self.lock = threading.Lock()
    self.executor = ThreadPoolExecutor(max_workers=max_threads)
    self.futures = []

  def crawl(self,url):
    with self.lock:
      if url in self.visited or urlparser(url).hostname != self.hostname:
        return
      self.visited.add(url)
      print(f"Crawling {url}")

    for child in self.htmlparser.parse(url):
      future = self.executor.submit(self.crawl,child)
      with self.lock:
        self.futures.append(future)


  def run(self,start_url):
    future = self.executor.submit(self.crawl,start_url)
    with self.lock:
      self.futures.append(future)
    wait(self.futures)
    self.executor.shutdown()
    return self.visited

crawlThreadPool= crawlthreadingwithThreadPool("http://site.com/a",htmlparser(test_graph_1))
st_time = time.time()
visited = crawlThreadPool.run("http://site.com/a")
end_time = time.time()
print(f"Time taken: {end_time - st_time} seconds")
print(visited)

'''
But your code does this:

Starts with one future for crawl(a)
That task submits more futures from inside threads
Those new futures are added to self.futures later
But wait(...) is already running and doesn’t know about them
So wait(self.futures) can return before the newly submitted tasks are finished, and the crawler may shut down prematurely.

ou're submitting future tasks asynchronously, and the list of self.futures keeps growing after wait() starts.
But wait(self.futures) only waits on what’s already in that list — it doesn't magically re-evaluate it.

 When You DO Need a Condition
You need a Condition when:

New work is discovered dynamically (like recursive crawling)
You don’t know how many total tasks there will be up front
You want to wait for all dynamic tasks to be “done”, not just submitted
'''

class ExecutorBasedCrawler:
    def __init__(self, start_url: str, htmlParser, max_workers: int = 10):
        self.start_url = start_url
        self.htmlParser = htmlParser
        self.hostname = urlparser(start_url).hostname
        self.visited = set()
        self.lock = threading.Lock()
        self.executor = ThreadPoolExecutor(max_workers=max_workers)
        self.active_tasks = 0
        self.condition = threading.Condition()

    def crawl_url(self, url: str):
        with self.lock:
            if url in self.visited or urlparser(url).hostname != self.hostname:
                return
            self.visited.add(url)

        with self.condition:
            self.active_tasks += 1

        print(f"Crawling {url}")
        try:
            children = self.htmlParser.parse(url)
            print(children)
        except Exception as e:
            print(f"Error fetching {url}: {e}")
            with self.condition:
                self.active_tasks -= 1
                self.condition.notify_all()
            return

        for child in children:
            self.executor.submit(self.crawl_url, child)

        with self.condition:
            self.active_tasks -= 1
            self.condition.notify_all()

    def run(self) -> Set[str]:
        self.executor.submit(self.crawl_url, self.start_url)
        with self.condition:
            while self.active_tasks > 0:
                self.condition.wait()
        self.executor.shutdown()
        return self.visited

exec_condition_crawler= ExecutorBasedCrawler("http://site.com/a",htmlparser(test_graph_1))
st_time = time.time()
visited = exec_condition_crawler.run()
end_time = time.time()
print(f"Time taken: {end_time - st_time} seconds")
print(visited)

import asyncio
from urllib.parse import urlparse

class HybridAsyncCrawler:
    def __init__(self):
        self.visited = set()
        self.lock = asyncio.Lock()
        self.queue = asyncio.Queue()

    async def downloadUrls(self, url, htmlParser, hostname, loop):
        # Run sync getUrls in a background thread (non-blocking for asyncio)
        next_urls = await loop.run_in_executor(None, htmlParser.parse, url)

        for u in next_urls:
            if urlparse(u).hostname == hostname:
                async with self.lock:
                    if u not in self.visited:
                        self.visited.add(u)
                        await self.queue.put(u)

    async def crawl(self, startUrl: str, htmlParser):
        loop = asyncio.get_event_loop()
        hostname = urlparse(startUrl).hostname
        self.visited = {startUrl}
        await self.queue.put(startUrl)

        tasks = []

        while not self.queue.empty() or tasks:
            while not self.queue.empty():
                url = await self.queue.get()
                task = asyncio.create_task(
                    self.downloadUrls(url, htmlParser, hostname, loop)
                )
                tasks.append(task)

            # Clean up finished tasks
            tasks = [t for t in tasks if not t.done()]
            await asyncio.sleep(0.01)  # Give other coroutines a chance to run

        return list(self.visited)

parser = htmlparser(test_graph_1)
crawler = HybridAsyncCrawler()
st_time = time.time()
# result = asyncio.run(crawler.crawl("http://site.com/a", parser))
result = await crawler.crawl("http://site.com/a", parser)
end_time = time.time()
print(f"Time taken: {end_time - st_time} seconds")
print("Visited URLs:", result)

class AsyncCrawler:
    def __init__(self):
        self.visited = set()
        self.lock = asyncio.Lock()
        self.que = asyncio.Queue()

    async def downloadUrls(self, url, htmlParser, hostname):
        next_urls = htmlParser.parse(url)  # assuming async getUrls ; here we don't have like that , we are not returning async result
        for u in next_urls:
            if urlparse(u).hostname == hostname:
                async with self.lock:
                    if u not in self.visited:
                        self.visited.add(u)
                        await self.que.put(u)

    async def crawl(self, startUrl: str, htmlParser):
        hostname = urlparse(startUrl).hostname
        self.visited = {startUrl}
        await self.que.put(startUrl)

        tasks = []

        while not self.que.empty() or tasks:
            while not self.que.empty():
                url = await self.que.get()
                task = asyncio.create_task(self.downloadUrls(url, htmlParser, hostname))
                tasks.append(task)

            tasks = [t for t in tasks if not t.done()]
            await asyncio.sleep(0.01)  # yield control to event loop

        return list(self.visited)

parser = htmlparser(test_graph_1)
crawler = AsyncCrawler()
st_time = time.time()
# result = asyncio.run(crawler.crawl("http://site.com/a", parser))
result = await crawler.crawl("http://site.com/a", parser)
end_time = time.time()
print(f"Time taken: {end_time - st_time} seconds")
print("Visited URLs:", result)