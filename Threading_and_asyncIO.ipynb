{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyND/ZryGsvHCnpcLuFJzSMb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tailorfenil/multithreading_and_asyncio/blob/main/Threading_and_asyncIO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from urllib.parse import urlparse as urlparser\n",
        "from typing import Set  # Import Set from typing module"
      ],
      "metadata": {
        "id": "eP3yp6iM_rlX"
      },
      "execution_count": 330,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define test \"webs\" as graphs\n",
        "test_graph_1 = {\n",
        "    \"http://site.com/a\": [\"http://site.com/b\", \"http://site.com/c\",\"http://site.com/d\"],\n",
        "    \"http://site.com/b\": [\"http://site.com/d\"],\n",
        "    \"http://site.com/c\": [],\n",
        "    \"http://site.com/d\": []\n",
        "}\n",
        "\n",
        "test_graph_2 = {\n",
        "    \"http://site.com/a\": [\"http://site.com/b\", \"http://othersite.com/e\"],\n",
        "    \"http://site.com/b\": [\"http://site.com/c\"],\n",
        "    \"http://site.com/c\": [],\n",
        "    \"http://othersite.com/e\": [\"http://othersite.com/f\"],\n",
        "    \"http://othersite.com/f\": []\n",
        "}\n"
      ],
      "metadata": {
        "id": "4x2WtTb8-f7g"
      },
      "execution_count": 279,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class htmlparser:\n",
        "\n",
        "  def __init__(self,web_crawler_map):\n",
        "    self.web_crawler_map = web_crawler_map\n",
        "\n",
        "  def parse(self,url:str) -> list[str]:\n",
        "    return self.web_crawler_map.get(url,[])"
      ],
      "metadata": {
        "id": "sy9tth1_9Yic"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 377,
      "metadata": {
        "id": "1wthiOb_33Dt"
      },
      "outputs": [],
      "source": [
        "\n",
        "class sequential_crawl:\n",
        "  def __init__(self,url:str,htmlparser):\n",
        "    self.url = url\n",
        "    self.htmlparser = htmlparser\n",
        "    self.visited = set()\n",
        "    self.hostname = urlparser(url).hostname\n",
        "\n",
        "  def crawl(self,url:str):\n",
        "    if url in self.visited or urlparser(url).hostname != self.hostname:\n",
        "      return\n",
        "    self.visited.add(url)\n",
        "    print(f\"Crawling {url}\")\n",
        "    for child in self.htmlparser.parse(url):\n",
        "      self.crawl(child)\n",
        "\n",
        "  def run(self,start_url:str) -> Set[str]:\n",
        "    self.crawl(start_url)\n",
        "    return self.visited\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "st_time = time.time()\n",
        "crawl = sequential_crawl(\"http://site.com/a\",htmlparser(test_graph_1))\n",
        "visited = crawl.run(\"http://site.com/a\")\n",
        "end_time = time.time()\n",
        "print(f\"Time taken: {end_time - st_time} seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jrsy_SN8-8SN",
        "outputId": "9ab3686a-2a2b-4b2e-df84-307cfca8d109"
      },
      "execution_count": 378,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Crawling http://site.com/a\n",
            "Crawling http://site.com/b\n",
            "Crawling http://site.com/d\n",
            "Crawling http://site.com/c\n",
            "Time taken: 0.00031638145446777344 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "sequential_crawl completes"
      ],
      "metadata": {
        "id": "2VfkbTBf_CdS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import threading\n",
        "class crawl_threading:\n",
        "  def __init__(self,url:str,htmlparser) -> None:\n",
        "    self.url=url\n",
        "    self.visited=set()\n",
        "    self.htmlparser = htmlparser\n",
        "    self.hostname = urlparser(url).hostname\n",
        "\n",
        "  def crawl(self,url:str):\n",
        "    if url in self.visited or urlparser(url).hostname != self.hostname:\n",
        "      return\n",
        "    self.visited.add(url)\n",
        "    print(f\"Crawling {url}\")\n",
        "\n",
        "    threads=[]\n",
        "    for child in self.htmlparser.parse(url):\n",
        "      thread = threading.Thread(target=self.crawl,args=(child,))\n",
        "      thread.start()\n",
        "      threads.append(thread)\n",
        "\n",
        "    for thread in threads:\n",
        "      thread.join()\n",
        ""
      ],
      "metadata": {
        "id": "_9I_jgkEA7hj"
      },
      "execution_count": 379,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "crawlthreads = crawl_threading(\"http://site.com/a\",htmlparser(test_graph_1))\n",
        "st_time = time.time()\n",
        "visite= crawlthreads.crawl(\"http://site.com/a\")\n",
        "end_time = time.time()\n",
        "print(f\"Time taken: {end_time - st_time} seconds\")\n",
        "print(visited)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mpsGVEadBhBH",
        "outputId": "d31dde2c-9101-4413-8d8e-2c53e01a897a"
      },
      "execution_count": 380,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Crawling http://site.com/a\n",
            "Crawling http://site.com/b\n",
            "Crawling http://site.com/d\n",
            "Crawling http://site.com/c\n",
            "Time taken: 0.0030410289764404297 seconds\n",
            "{'http://site.com/a', 'http://site.com/d', 'http://site.com/b', 'http://site.com/c'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "crawl_threading completes"
      ],
      "metadata": {
        "id": "LptdjPs3_Eoy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class crawl_thread_safe:\n",
        "  def __init__(self,url:str,htmlparser) -> None:\n",
        "    self.url=url\n",
        "    self.visited=set()\n",
        "    self.htmlparser = htmlparser\n",
        "    self.hostname = urlparser(url).hostname\n",
        "    self.lock = threading.Lock()\n",
        "\n",
        "  def crawl(self,url:str):\n",
        "    with self.lock:\n",
        "      if url in self.visited or urlparser(url).hostname != self.hostname:\n",
        "        return\n",
        "      self.visited.add(url)\n",
        "      print(f\"Crawling {url}\")\n",
        "\n",
        "    threads=[]\n",
        "\n",
        "    for child in self.htmlparser.parse(url):\n",
        "      thread = threading.Thread(target=self.crawl,args=(child,))\n",
        "      thread.start()\n",
        "      threads.append(thread)\n",
        "\n",
        "    for thread in threads:\n",
        "      thread.join()\n",
        "\n",
        "  def run(self,start_url:str) -> Set[str]:\n",
        "    self.crawl(start_url)\n",
        "    return self.visited"
      ],
      "metadata": {
        "id": "ekz2jzTUCvDE"
      },
      "execution_count": 381,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "st_time = time.time()\n",
        "crawlthreadsafe = crawl_thread_safe(\"http://site.com/a\",htmlparser(test_graph_1))\n",
        "visited = crawlthreadsafe.run(\"http://site.com/a\")\n",
        "end_time = time.time()\n",
        "print(f\"Time taken: {end_time - st_time} seconds\")\n",
        "print(visited)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KaHNLU7eDLos",
        "outputId": "3f618441-6f49-4638-feb2-5d10cfc7dbb0"
      },
      "execution_count": 382,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Crawling http://site.com/a\n",
            "Crawling http://site.com/b\n",
            "Crawling http://site.com/d\n",
            "Crawling http://site.com/c\n",
            "Time taken: 0.0024394989013671875 seconds\n",
            "{'http://site.com/a', 'http://site.com/d', 'http://site.com/b', 'http://site.com/c'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "crawl_thread_safe completes"
      ],
      "metadata": {
        "id": "GUE86BRc_InM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class crawl_thread_safe_concurrrency_limit:\n",
        "  def __init__(self,url:str,htmlparser,max_threads:int=10) -> None:\n",
        "    self.url=url\n",
        "    self.visited=set()\n",
        "    self.htmlparser = htmlparser\n",
        "    self.hostname = urlparser(url).hostname\n",
        "    self.lock= threading.Lock()\n",
        "    self.semaphore = threading.Semaphore(max_threads)\n",
        "\n",
        "  def crawl(self,url:str):\n",
        "    with self.semaphore:\n",
        "      with self.lock:\n",
        "        if url in self.visited or urlparser(url).hostname != self.hostname:\n",
        "          return\n",
        "        self.visited.add(url)\n",
        "        print(f\"Crawling {url}\")\n",
        "\n",
        "      threads = []\n",
        "      for child in self.htmlparser.parse(url):\n",
        "        thread = threading.Thread(target=self.crawl,args=(child,))\n",
        "        thread.start()\n",
        "        threads.append(thread)\n",
        "\n",
        "      for thread in threads:\n",
        "        thread.join()\n"
      ],
      "metadata": {
        "id": "5t9srk5WExcw"
      },
      "execution_count": 383,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "crawl_concurrency_limit = crawl_thread_safe_concurrrency_limit(\"http://site.com/a\",htmlparser(test_graph_1))\n",
        "st_time = time.time()\n",
        "visited = crawl_concurrency_limit.crawl(\"http://site.com/a\")\n",
        "end_time = time.time()\n",
        "print(f\"Time taken: {end_time - st_time} seconds\")\n",
        "print(visited)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nFzcIBzsFLhB",
        "outputId": "e54a0481-e983-4b2b-d080-7e163dce710a"
      },
      "execution_count": 384,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Crawling http://site.com/a\n",
            "Crawling http://site.com/b\n",
            "Crawling http://site.com/d\n",
            "Crawling http://site.com/c\n",
            "Time taken: 0.0018458366394042969 seconds\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "crawl_thread_safe_concurrrency_limit completes"
      ],
      "metadata": {
        "id": "lAeZpD07_LVK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from concurrent.futures import ThreadPoolExecutor, wait\n",
        "class crawlthreadingwithThreadPool:\n",
        "  def __init__(self,url,htmlparser,max_threads:int=10):\n",
        "    self.url=url\n",
        "    self.htmlparser= htmlparser\n",
        "    self.visited=set()\n",
        "    self.hostname = urlparser(url).hostname\n",
        "    self.lock = threading.Lock()\n",
        "    self.executor = ThreadPoolExecutor(max_workers=max_threads)\n",
        "    self.futures = []\n",
        "\n",
        "  def crawl(self,url):\n",
        "    with self.lock:\n",
        "      if url in self.visited or urlparser(url).hostname != self.hostname:\n",
        "        return\n",
        "      self.visited.add(url)\n",
        "      print(f\"Crawling {url}\")\n",
        "\n",
        "    for child in self.htmlparser.parse(url):\n",
        "      future = self.executor.submit(self.crawl,child)\n",
        "      with self.lock:\n",
        "        self.futures.append(future)\n",
        "\n",
        "\n",
        "  def run(self,start_url):\n",
        "    future = self.executor.submit(self.crawl,start_url)\n",
        "    with self.lock:\n",
        "      self.futures.append(future)\n",
        "    wait(self.futures)\n",
        "    self.executor.shutdown()\n",
        "    return self.visited"
      ],
      "metadata": {
        "id": "Refs60NvKkW-"
      },
      "execution_count": 385,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "crawlThreadPool= crawlthreadingwithThreadPool(\"http://site.com/a\",htmlparser(test_graph_1))\n",
        "st_time = time.time()\n",
        "visited = crawlThreadPool.run(\"http://site.com/a\")\n",
        "end_time = time.time()\n",
        "print(f\"Time taken: {end_time - st_time} seconds\")\n",
        "print(visited)\n",
        "\n",
        "'''\n",
        "But your code does this:\n",
        "\n",
        "Starts with one future for crawl(a)\n",
        "That task submits more futures from inside threads\n",
        "Those new futures are added to self.futures later\n",
        "But wait(...) is already running and doesn’t know about them\n",
        "So wait(self.futures) can return before the newly submitted tasks are finished, and the crawler may shut down prematurely.\n",
        "\n",
        "ou're submitting future tasks asynchronously, and the list of self.futures keeps growing after wait() starts.\n",
        "But wait(self.futures) only waits on what’s already in that list — it doesn't magically re-evaluate it.\n",
        "\n",
        " When You DO Need a Condition\n",
        "You need a Condition when:\n",
        "\n",
        "New work is discovered dynamically (like recursive crawling)\n",
        "You don’t know how many total tasks there will be up front\n",
        "You want to wait for all dynamic tasks to be “done”, not just submitted\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 239
        },
        "id": "_uL6KbKyMmqy",
        "outputId": "cbd97c9a-7a6c-4196-d678-de71d73e1348"
      },
      "execution_count": 400,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Crawling http://site.com/a\n",
            "Crawling http://site.com/b\n",
            "Crawling http://site.com/c\n",
            "Crawling http://site.com/d\n",
            "Time taken: 0.0026743412017822266 seconds\n",
            "{'http://site.com/a', 'http://site.com/d', 'http://site.com/b', 'http://site.com/c'}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nBut your code does this:\\n\\nStarts with one future for crawl(a)\\nThat task submits more futures from inside threads\\nThose new futures are added to self.futures later\\nBut wait(...) is already running and doesn’t know about them\\nSo wait(self.futures) can return before the newly submitted tasks are finished, and the crawler may shut down prematurely.\\n\\nou're submitting future tasks asynchronously, and the list of self.futures keeps growing after wait() starts.\\nBut wait(self.futures) only waits on what’s already in that list — it doesn't magically re-evaluate it.\\n\\n When You DO Need a Condition\\nYou need a Condition when:\\n\\nNew work is discovered dynamically (like recursive crawling)\\nYou don’t know how many total tasks there will be up front\\nYou want to wait for all dynamic tasks to be “done”, not just submitted\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 400
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "crawlthreadingwithThreadPool completes"
      ],
      "metadata": {
        "id": "r4nTqjXg_PAj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ExecutorBasedCrawler:\n",
        "    def __init__(self, start_url: str, htmlParser, max_workers: int = 10):\n",
        "        self.start_url = start_url\n",
        "        self.htmlParser = htmlParser\n",
        "        self.hostname = urlparser(start_url).hostname\n",
        "        self.visited = set()\n",
        "        self.lock = threading.Lock()\n",
        "        self.executor = ThreadPoolExecutor(max_workers=max_workers)\n",
        "        self.active_tasks = 0\n",
        "        self.condition = threading.Condition()\n",
        "\n",
        "    def crawl_url(self, url: str):\n",
        "        with self.lock:\n",
        "            if url in self.visited or urlparser(url).hostname != self.hostname:\n",
        "                return\n",
        "            self.visited.add(url)\n",
        "\n",
        "        with self.condition:\n",
        "            self.active_tasks += 1\n",
        "\n",
        "        print(f\"Crawling {url}\")\n",
        "        try:\n",
        "            children = self.htmlParser.parse(url)\n",
        "            print(children)\n",
        "        except Exception as e:\n",
        "            print(f\"Error fetching {url}: {e}\")\n",
        "            with self.condition:\n",
        "                self.active_tasks -= 1\n",
        "                self.condition.notify_all()\n",
        "            return\n",
        "\n",
        "        for child in children:\n",
        "            self.executor.submit(self.crawl_url, child)\n",
        "\n",
        "        with self.condition:\n",
        "            self.active_tasks -= 1\n",
        "            self.condition.notify_all()\n",
        "\n",
        "    def run(self) -> Set[str]:\n",
        "        self.executor.submit(self.crawl_url, self.start_url)\n",
        "        with self.condition:\n",
        "            while self.active_tasks > 0:\n",
        "                self.condition.wait()\n",
        "        self.executor.shutdown()\n",
        "        return self.visited\n"
      ],
      "metadata": {
        "id": "8wYCqd-9XH5i"
      },
      "execution_count": 394,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "exec_condition_crawler= ExecutorBasedCrawler(\"http://site.com/a\",htmlparser(test_graph_1))\n",
        "st_time = time.time()\n",
        "visited = exec_condition_crawler.run()\n",
        "end_time = time.time()\n",
        "print(f\"Time taken: {end_time - st_time} seconds\")\n",
        "print(visited)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TCNLFlv0YxI1",
        "outputId": "39105f79-7070-45a2-c17b-bcb6cf36d5c9"
      },
      "execution_count": 397,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Crawling http://site.com/a\n",
            "['http://site.com/b', 'http://site.com/c', 'http://site.com/d']\n",
            "Crawling http://site.com/b\n",
            "['http://site.com/d']\n",
            "Crawling http://site.com/c\n",
            "[]\n",
            "Crawling http://site.com/d\n",
            "[]\n",
            "Time taken: 0.002387523651123047 seconds\n",
            "{'http://site.com/a', 'http://site.com/d', 'http://site.com/b', 'http://site.com/c'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ExecutorBasedCrawler completes"
      ],
      "metadata": {
        "id": "FrmXWHu5_S35"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "from urllib.parse import urlparse\n",
        "\n",
        "class HybridAsyncCrawler:\n",
        "    def __init__(self):\n",
        "        self.visited = set()\n",
        "        self.lock = asyncio.Lock()\n",
        "        self.queue = asyncio.Queue()\n",
        "\n",
        "    async def downloadUrls(self, url, htmlParser, hostname, loop):\n",
        "        # Run sync getUrls in a background thread (non-blocking for asyncio)\n",
        "        next_urls = await loop.run_in_executor(None, htmlParser.parse, url)\n",
        "\n",
        "        for u in next_urls:\n",
        "            if urlparse(u).hostname == hostname:\n",
        "                async with self.lock:\n",
        "                    if u not in self.visited:\n",
        "                        self.visited.add(u)\n",
        "                        await self.queue.put(u)\n",
        "\n",
        "    async def crawl(self, startUrl: str, htmlParser):\n",
        "        loop = asyncio.get_event_loop()\n",
        "        hostname = urlparse(startUrl).hostname\n",
        "        self.visited = {startUrl}\n",
        "        await self.queue.put(startUrl)\n",
        "\n",
        "        tasks = []\n",
        "\n",
        "        while not self.queue.empty() or tasks:\n",
        "            while not self.queue.empty():\n",
        "                url = await self.queue.get()\n",
        "                task = asyncio.create_task(\n",
        "                    self.downloadUrls(url, htmlParser, hostname, loop)\n",
        "                )\n",
        "                tasks.append(task)\n",
        "\n",
        "            # Clean up finished tasks\n",
        "            tasks = [t for t in tasks if not t.done()]\n",
        "            await asyncio.sleep(0.01)  # Give other coroutines a chance to run\n",
        "\n",
        "        return list(self.visited)"
      ],
      "metadata": {
        "id": "98jcpXPD7sV7"
      },
      "execution_count": 408,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parser = htmlparser(test_graph_1)\n",
        "crawler = HybridAsyncCrawler()\n",
        "st_time = time.time()\n",
        "# result = asyncio.run(crawler.crawl(\"http://site.com/a\", parser))\n",
        "result = await crawler.crawl(\"http://site.com/a\", parser)\n",
        "end_time = time.time()\n",
        "print(f\"Time taken: {end_time - st_time} seconds\")\n",
        "print(\"Visited URLs:\", result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KUxa14AV73-q",
        "outputId": "6d505a3f-4a01-4c39-cd92-45122ed40a2e"
      },
      "execution_count": 415,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time taken: 0.03406953811645508 seconds\n",
            "Visited URLs: ['http://site.com/a', 'http://site.com/d', 'http://site.com/b', 'http://site.com/c']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "HybridAsyncCrawler completes"
      ],
      "metadata": {
        "id": "3Fl-vd39_WLF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AsyncCrawler:\n",
        "    def __init__(self):\n",
        "        self.visited = set()\n",
        "        self.lock = asyncio.Lock()\n",
        "        self.que = asyncio.Queue()\n",
        "\n",
        "    async def downloadUrls(self, url, htmlParser, hostname):\n",
        "        next_urls = htmlParser.parse(url)  # assuming async getUrls ; here we don't have like that , we are not returning async result\n",
        "        for u in next_urls:\n",
        "            if urlparse(u).hostname == hostname:\n",
        "                async with self.lock:\n",
        "                    if u not in self.visited:\n",
        "                        self.visited.add(u)\n",
        "                        await self.que.put(u)\n",
        "\n",
        "    async def crawl(self, startUrl: str, htmlParser):\n",
        "        hostname = urlparse(startUrl).hostname\n",
        "        self.visited = {startUrl}\n",
        "        await self.que.put(startUrl)\n",
        "\n",
        "        tasks = []\n",
        "\n",
        "        while not self.que.empty() or tasks:\n",
        "            while not self.que.empty():\n",
        "                url = await self.que.get()\n",
        "                task = asyncio.create_task(self.downloadUrls(url, htmlParser, hostname))\n",
        "                tasks.append(task)\n",
        "\n",
        "            tasks = [t for t in tasks if not t.done()]\n",
        "            await asyncio.sleep(0.01)  # yield control to event loop\n",
        "\n",
        "        return list(self.visited)"
      ],
      "metadata": {
        "id": "qelXXvxC83rD"
      },
      "execution_count": 420,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parser = htmlparser(test_graph_1)\n",
        "crawler = AsyncCrawler()\n",
        "st_time = time.time()\n",
        "# result = asyncio.run(crawler.crawl(\"http://site.com/a\", parser))\n",
        "result = await crawler.crawl(\"http://site.com/a\", parser)\n",
        "end_time = time.time()\n",
        "print(f\"Time taken: {end_time - st_time} seconds\")\n",
        "print(\"Visited URLs:\", result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9oleY07B85oo",
        "outputId": "6ccb49fc-9cd1-4c3d-e6e6-7b1b814ae0b0"
      },
      "execution_count": 421,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time taken: 0.03393411636352539 seconds\n",
            "Visited URLs: ['http://site.com/a', 'http://site.com/d', 'http://site.com/b', 'http://site.com/c']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "AsyncCrawler completes"
      ],
      "metadata": {
        "id": "CqGXty1Z_aVv"
      }
    }
  ]
}